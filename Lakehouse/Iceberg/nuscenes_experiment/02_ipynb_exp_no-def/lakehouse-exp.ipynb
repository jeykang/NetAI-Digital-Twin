{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a192eda6",
   "metadata": {},
   "source": [
    "# Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99aa5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/19 12:09:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/19 12:09:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# [PART 1] Environment & Spark Init\n",
    "# =============================================================================\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "s3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio:9000\")\n",
    "nessie_uri = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v1\")\n",
    "RAW_DATA_PATH = \"/mnt/kkr/iceberg/datasets/nuscenes_v1.0-mini/v1.0-mini\" \n",
    "\n",
    "if not aws_access_key or not aws_secret_key:\n",
    "    print(\"Error: AWS Access Key or Secret Key is missing in environment variables.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NessieMinioSpark\") \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.defaultCatalog', 'spark_catalog') \\\n",
    "    .config('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.nessie.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.nessie.ref', 'main') \\\n",
    "    .config('spark.sql.catalog.nessie.cache-enabled', 'false') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.region', aws_region) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.access-key-id', aws_access_key) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.secret-access-key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', aws_access_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.endpoint', s3_endpoint) \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672314b6",
   "metadata": {},
   "source": [
    "## Iceberg-Silver Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ce1c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Experiment] Scaling Key Tables by 1x (others keep 1x) ...\n",
      "Data Ingestion Finished: 1.63s\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTOR = 1\n",
    "\n",
    "setup_start = time.time()\n",
    "\n",
    "# Namespace 생성\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.nusc_db\")\n",
    "\n",
    "try:\n",
    "    # 테이블이 이미 있는지 확인 (테스트용)\n",
    "    # spark.table(\"nessie.nusc_db.sample_data\") # 이 부분은 주석 처리하거나 에러 핸들링을 위해 둠\n",
    "    # print(\"Tables might exist. Attempting to overwrite...\")\n",
    "    pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 1. JSON 읽기\n",
    "df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "# 2. Pre-Join (Denormalization)\n",
    "df_channel_map = df_calibrated.join(\n",
    "    df_sensor, \n",
    "    df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]\n",
    ").select(\n",
    "    df_calibrated[\"token\"].alias(\"calib_token\"), \n",
    "    df_sensor[\"channel\"]\n",
    ")\n",
    "\n",
    "df_sample_data_enriched = df_sample_data.join(\n",
    "    df_channel_map,\n",
    "    df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]\n",
    ").drop(\"calib_token\")\n",
    "\n",
    "# =========================================================\n",
    "# [실험 변수] 데이터 스케일 팩터\n",
    "# =========================================================\n",
    "\n",
    "def scale_df(df, factor):\n",
    "    if factor <= 1: return df\n",
    "    return df.crossJoin(spark.range(factor)).drop(\"id\")\n",
    "\n",
    "print(f\">>> [Experiment] Scaling Key Tables by {SCALE_FACTOR}x (others keep 1x) ...\")\n",
    "\n",
    "# 1. 참조 테이블 (Reference Tables) - 스케일링 하지 않음 (Python Dictionary 동작 모사)\n",
    "# samples, instances, category는 1배 유지\n",
    "df_sample.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"nessie.nusc_db.samples\")\n",
    "df_category.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"nessie.nusc_db.category\")\n",
    "df_instance.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"nessie.nusc_db.instances\")\n",
    "\n",
    "# 2. 팩트 테이블 (Fact Tables) - 스케일링 적용 (데이터 폭증 유발)\n",
    "# sample_data와 annotations만 늘려서 10 * 10 = 100배 효과를 냄\n",
    "scale_df(df_sample_data_enriched, SCALE_FACTOR).write.format(\"iceberg\") \\\n",
    "    .partitionBy(\"channel\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"nessie.nusc_db.sample_data\")\n",
    "\n",
    "scale_df(df_annotation, SCALE_FACTOR).write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"nessie.nusc_db.annotations\")\n",
    "\n",
    "print(f\"Data Ingestion Finished: {time.time() - setup_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9dc59",
   "metadata": {},
   "source": [
    "### Iceberg-Silver Query Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c27db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2533초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2611초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2132초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2350초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2562초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2538초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2577초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2259초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2606초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.2280초\n",
      "Total avg Time Elapsed: 0.2445 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=1\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "    query_start = time.time()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sd.filename as img_path,\n",
    "        a.translation,\n",
    "        a.size,\n",
    "        a.rotation\n",
    "    FROM nessie.nusc_db.samples s\n",
    "    JOIN nessie.nusc_db.sample_data sd \n",
    "        ON s.token = sd.sample_token\n",
    "    JOIN nessie.nusc_db.annotations a \n",
    "        ON s.token = a.sample_token\n",
    "    JOIN nessie.nusc_db.instances i \n",
    "        ON a.instance_token = i.token\n",
    "    JOIN nessie.nusc_db.category c \n",
    "        ON i.category_token = c.token\n",
    "    WHERE \n",
    "        sd.channel = 'CAM_FRONT' \n",
    "        AND c.name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "    print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "    #spark.stop()\n",
    "    all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d05851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4549초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4696초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4230초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.3984초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4220초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4307초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.3822초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4702초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4115초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.4908초\n",
      "Total avg Time Elapsed: 0.4353 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=3\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "    query_start = time.time()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sd.filename as img_path,\n",
    "        a.translation,\n",
    "        a.size,\n",
    "        a.rotation\n",
    "    FROM nessie.nusc_db.samples s\n",
    "    JOIN nessie.nusc_db.sample_data sd \n",
    "        ON s.token = sd.sample_token\n",
    "    JOIN nessie.nusc_db.annotations a \n",
    "        ON s.token = a.sample_token\n",
    "    JOIN nessie.nusc_db.instances i \n",
    "        ON a.instance_token = i.token\n",
    "    JOIN nessie.nusc_db.category c \n",
    "        ON i.category_token = c.token\n",
    "    WHERE \n",
    "        sd.channel = 'CAM_FRONT' \n",
    "        AND c.name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "    print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "    #spark.stop()\n",
    "    all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddfe1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6546초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6339초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.8451초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.7130초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6355초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6601초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6182초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.5970초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6147초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.6150초\n",
      "Total avg Time Elapsed: 0.6587 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=5\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "    query_start = time.time()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sd.filename as img_path,\n",
    "        a.translation,\n",
    "        a.size,\n",
    "        a.rotation\n",
    "    FROM nessie.nusc_db.samples s\n",
    "    JOIN nessie.nusc_db.sample_data sd \n",
    "        ON s.token = sd.sample_token\n",
    "    JOIN nessie.nusc_db.annotations a \n",
    "        ON s.token = a.sample_token\n",
    "    JOIN nessie.nusc_db.instances i \n",
    "        ON a.instance_token = i.token\n",
    "    JOIN nessie.nusc_db.category c \n",
    "        ON i.category_token = c.token\n",
    "    WHERE \n",
    "        sd.channel = 'CAM_FRONT' \n",
    "        AND c.name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "    print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "    #spark.stop()\n",
    "    all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "338ea1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 1.0477초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 1.0171초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9256초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 1.0421초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9735초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9722초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9497초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9177초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9279초\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.9071초\n",
      "Total avg Time Elapsed: 0.9681 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#SF=7\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "    query_start = time.time()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sd.filename as img_path,\n",
    "        a.translation,\n",
    "        a.size,\n",
    "        a.rotation\n",
    "    FROM nessie.nusc_db.samples s\n",
    "    JOIN nessie.nusc_db.sample_data sd \n",
    "        ON s.token = sd.sample_token\n",
    "    JOIN nessie.nusc_db.annotations a \n",
    "        ON s.token = a.sample_token\n",
    "    JOIN nessie.nusc_db.instances i \n",
    "        ON a.instance_token = i.token\n",
    "    JOIN nessie.nusc_db.category c \n",
    "        ON i.category_token = c.token\n",
    "    WHERE \n",
    "        sd.channel = 'CAM_FRONT' \n",
    "        AND c.name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "    print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "    #spark.stop()\n",
    "    all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7fe75",
   "metadata": {},
   "source": [
    "## Iceberg-Gold Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fabf893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 1: Gold Table 생성 (Join + Denormalization)\n",
      ">>> [Experiment] Scaling Gold Table by 7x7=100x ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 925:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Table Ingestion Finished: 81.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "SCALE_FACTOR = 7\n",
    "\n",
    "print(\">>> [Lakehouse] Phase 1: Gold Table 생성 (Join + Denormalization)\")\n",
    "setup_start = time.time()\n",
    "\n",
    "# 1. JSON 읽기\n",
    "df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "# 2. 복잡한 조인을 미리 수행 (Denormalization)\n",
    "# CAM_FRONT 채널 정보 결합\n",
    "df_channel_map = df_calibrated.join(df_sensor, df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]) \\\n",
    "    .select(df_calibrated[\"token\"].alias(\"calib_token\"), df_sensor[\"channel\"])\n",
    "\n",
    "# 전체 조인 수행 (Gold Table용 데이터 구성)\n",
    "df_gold_raw = df_sample_data.join(df_channel_map, df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]) \\\n",
    "    .join(df_annotation, df_sample_data[\"sample_token\"] == df_annotation[\"sample_token\"]) \\\n",
    "    .join(df_instance, df_annotation[\"instance_token\"] == df_instance[\"token\"]) \\\n",
    "    .join(df_category, df_instance[\"category_token\"] == df_category[\"token\"]) \\\n",
    "    .select(\n",
    "        df_sample_data[\"filename\"].alias(\"img_path\"),\n",
    "        df_annotation[\"translation\"],\n",
    "        df_annotation[\"size\"],\n",
    "        df_annotation[\"rotation\"],\n",
    "        df_sensor[\"channel\"],\n",
    "        df_category[\"name\"].alias(\"category_name\")\n",
    "    )\n",
    "\n",
    "# 3. 데이터 스케일링 (10배 증강 -> 조인 폭발 시뮬레이션 결과와 맞추기 위해 100배 효과 적용 가능)\n",
    "def scale_df(df, factor):\n",
    "    if factor <= 1: return df\n",
    "    # Baseline과 동일하게 10x10=100배 효과를 내기 위해 factor*factor로 증강\n",
    "    return df.crossJoin(spark.range(factor * factor)).drop(\"id\")\n",
    "\n",
    "print(f\">>> [Experiment] Scaling Gold Table by {SCALE_FACTOR}x{SCALE_FACTOR}=100x ...\")\n",
    "df_gold_final = scale_df(df_gold_raw, SCALE_FACTOR)\n",
    "\n",
    "# 4. Iceberg Gold Table 저장\n",
    "# CAM_FRONT나 Category에 상관없이 일단 저장한 뒤 쿼리에서 필터링하는 방식이 실용적입니다.\n",
    "df_gold_final.write.format(\"iceberg\") \\\n",
    "    .partitionBy(\"channel\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"nessie.nusc_db.gold_train_set\")\n",
    "\n",
    "print(f\"Gold Table Ingestion Finished: {time.time() - setup_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbdfc6",
   "metadata": {},
   "source": [
    "### Iceberg-Gold Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10a2168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0961초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0624초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0524초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0635초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0615초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0822초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0737초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0713초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0561초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0561초\n",
      "Total avg Time Elapsed: 0.0675 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=1\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "  print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "  query_start = time.time()\n",
    "\n",
    "  # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "  query = \"\"\"\n",
    "  SELECT img_path, translation, size, rotation\n",
    "  FROM nessie.nusc_db.gold_train_set\n",
    "  WHERE channel = 'CAM_FRONT' \n",
    "    AND category_name = 'human.pedestrian.adult'\n",
    "  \"\"\"\n",
    "\n",
    "  result_df = spark.sql(query)\n",
    "  count = result_df.count()\n",
    "\n",
    "  query_end = time.time()\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "  #spark.stop()\n",
    "  all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f03c6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1191초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1186초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1244초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1105초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0925초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0991초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0818초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0773초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0803초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0889초\n",
      "Total avg Time Elapsed: 0.0993 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=3\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "  print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "  query_start = time.time()\n",
    "\n",
    "  # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "  query = \"\"\"\n",
    "  SELECT img_path, translation, size, rotation\n",
    "  FROM nessie.nusc_db.gold_train_set\n",
    "  WHERE channel = 'CAM_FRONT' \n",
    "    AND category_name = 'human.pedestrian.adult'\n",
    "  \"\"\"\n",
    "\n",
    "  result_df = spark.sql(query)\n",
    "  count = result_df.count()\n",
    "\n",
    "  query_end = time.time()\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "  #spark.stop()\n",
    "  all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3134ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1335초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1295초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0913초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.0959초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1298초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1296초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1442초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1408초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1316초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1242초\n",
      "Total avg Time Elapsed: 0.1250 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=5\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "  print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "  query_start = time.time()\n",
    "\n",
    "  # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "  query = \"\"\"\n",
    "  SELECT img_path, translation, size, rotation\n",
    "  FROM nessie.nusc_db.gold_train_set\n",
    "  WHERE channel = 'CAM_FRONT' \n",
    "    AND category_name = 'human.pedestrian.adult'\n",
    "  \"\"\"\n",
    "\n",
    "  result_df = spark.sql(query)\n",
    "  count = result_df.count()\n",
    "\n",
    "  query_end = time.time()\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "  #spark.stop()\n",
    "  all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00518e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1727초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1768초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1735초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1396초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1388초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1097초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1581초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1319초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1439초\n",
      ">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 0.1522초\n",
      "Total avg Time Elapsed: 0.1497 sec\n"
     ]
    }
   ],
   "source": [
    "#SF=7\n",
    "all_time=0\n",
    "test_count=10\n",
    "for i in range(test_count):\n",
    "  print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "  query_start = time.time()\n",
    "\n",
    "  # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "  query = \"\"\"\n",
    "  SELECT img_path, translation, size, rotation\n",
    "  FROM nessie.nusc_db.gold_train_set\n",
    "  WHERE channel = 'CAM_FRONT' \n",
    "    AND category_name = 'human.pedestrian.adult'\n",
    "  \"\"\"\n",
    "\n",
    "  result_df = spark.sql(query)\n",
    "  count = result_df.count()\n",
    "\n",
    "  query_end = time.time()\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")\n",
    "\n",
    "  #spark.stop()\n",
    "  all_time += query_end - query_start\n",
    "print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9096a8",
   "metadata": {},
   "source": [
    "# Lakehouse General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc234c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 2: 실험 시작 - 모델 학습 데이터셋 구성 쿼리\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 2748300건 추출 완료\n",
      ">>> [Lakehouse] 데이터셋 구성 소요 시간: 1.6202초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Phase 2: Experiment (Query)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\">>> [Lakehouse] Phase 2: 실험 시작 - 모델 학습 데이터셋 구성 쿼리\")\n",
    "query_start = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    sd.filename as img_path,\n",
    "    a.translation,\n",
    "    a.size,\n",
    "    a.rotation\n",
    "FROM nessie.nusc_db.samples s\n",
    "JOIN nessie.nusc_db.sample_data sd \n",
    "    ON s.token = sd.sample_token\n",
    "JOIN nessie.nusc_db.annotations a \n",
    "    ON s.token = a.sample_token\n",
    "JOIN nessie.nusc_db.instances i \n",
    "    ON a.instance_token = i.token\n",
    "JOIN nessie.nusc_db.category c \n",
    "    ON i.category_token = c.token\n",
    "WHERE \n",
    "    sd.channel = 'CAM_FRONT' \n",
    "    AND c.name = 'human.pedestrian.adult'\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "count = result_df.count()\n",
    "\n",
    "query_end = time.time()\n",
    "print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "print(f\">>> [Lakehouse] 데이터셋 구성 소요 시간: {query_end - query_start:.4f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d90c5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# KAIST E2E Dataset Ingestion Plan for Apache Iceberg Lakehouse

## Executive Summary

This document outlines a comprehensive plan to ingest the KAIST/MOTIE E2E autonomous driving dataset into the Apache Iceberg Lakehouse. The plan is informed by the existing nuScenes ingestion patterns and the performance optimization strategies (Silver/Gold table architecture) already proven in the codebase.

---

## 1. Schema Analysis & Data Model Assessment

### 1.1 KAIST Schema Structure Overview

The KAIST schema follows a **hierarchical structure** similar to nuScenes but with some key differences:

```
Session (Top-level)
  └── Clip (Recording segment)
       ├── Frame (Synchronized sensor snapshot)
       │    ├── Camera (multiple named sensors)
       │    ├── Lidar
       │    ├── Radar (multiple named sensors)
       │    ├── DynamicObject (3D annotations)
       │    ├── Occupancy
       │    ├── Motion
       │    └── EgoMotion
       ├── Calibration (per-sensor)
       └── HDMap (geographic context)
```

### 1.2 Key Differences from nuScenes

| Aspect | nuScenes | KAIST |
|--------|----------|-------|
| Top-level | `scene` → `sample` | `Session` → `Clip` → `Frame` |
| Hierarchy depth | 2 levels | 3 levels |
| Calibration scope | Per-sample | Per-clip |
| Annotation types | Single (sample_annotation) | Multiple (DynamicObject, Occupancy, Motion) |
| Ego motion | Tied to sample_data | Dual level (Frame + Session summary) |
| Map linking | Via log.map_token | Direct clip-level HDMap reference |

### 1.3 Identified Access Patterns

Based on autonomous driving ML workloads (training, inference, evaluation):

1. **Temporal Queries**: Retrieve frames within a time range for a specific session/clip
2. **Sensor-Type Queries**: Filter by sensor modality (Camera/Lidar/Radar)
3. **Geographic Queries**: Filter by city/site (via HDMap)
4. **Annotation Queries**: Retrieve objects of specific categories
5. **Calibration Lookup**: Join sensor data with calibration for projection

---

## 2. Optimal Storage Strategy

### 2.1 Table Architecture: Medallion Pattern

Following the proven pattern from the nuScenes experiments, we'll implement a **Bronze → Silver → Gold** medallion architecture:

#### Bronze Layer (Raw Ingestion)
- **Purpose**: 1:1 mapping from source files, preserve original structure
- **Use case**: Data lineage, debugging, schema evolution
- **Tables**: Direct mapping of each KAIST table

#### Silver Layer (Normalized + Optimized)
- **Purpose**: Cleaned, typed, and partitioned for efficient joins
- **Use case**: Ad-hoc analytics, data exploration, Superset dashboards
- **Optimization**: Strategic partitioning on high-cardinality join keys

#### Gold Layer (Denormalized)
- **Purpose**: Pre-joined, ML-ready feature tables
- **Use case**: Training data loaders, inference pipelines
- **Optimization**: Zero-join reads with partition pruning

### 2.2 Partitioning Strategy

| Table | Bronze Partition | Silver Partition | Gold Partition |
|-------|------------------|------------------|----------------|
| Session | None | None | N/A (small table) |
| Clip | None | `session_id` | N/A |
| Frame | None | `clip_id` | N/A |
| Camera | None | `camera_name`, `clip_id` | `camera_name` |
| Lidar | None | `clip_id` | N/A |
| Radar | None | `radar_name`, `clip_id` | `radar_name` |
| DynamicObject | None | `clip_id` | `category` (via join) |
| Calibration | None | `clip_id`, `sensor_name` | N/A |

### 2.3 Sort Order & Clustering

For optimal scan performance, use Iceberg's **sort order** feature:

```sql
-- Example: Camera table optimized for temporal queries
ALTER TABLE kaist.silver.camera 
WRITE ORDERED BY clip_id, frame_id, sensor_timestamp;
```

This enables efficient range scans when querying frames within a time window.

---

## 3. Gold Table Designs (Pre-computed Feature Tables)

### 3.1 `gold.camera_annotations`

**Purpose**: Front-camera images with 3D object annotations (most common ML training query)

| Column | Type | Source |
|--------|------|--------|
| `frame_id` | string | Frame |
| `clip_id` | string | Frame |
| `session_id` | string | Session |
| `sensor_timestamp` | long | Camera |
| `camera_name` | string | Camera |
| `filename` | string | Camera |
| `extrinsics` | struct | Calibration |
| `camera_intrinsics` | struct | Calibration |
| `boxes_3d` | array<struct> | DynamicObject |
| `categories` | array<string> | Category (via DynamicObject) |
| `city` | string | HDMap |

**Partition By**: `camera_name`  
**Sort By**: `session_id`, `clip_id`, `frame_id`

### 3.2 `gold.lidar_with_ego`

**Purpose**: LiDAR point clouds with ego-motion for SLAM/localization

| Column | Type | Source |
|--------|------|--------|
| `frame_id` | string | Frame |
| `clip_id` | string | Frame |
| `filename` | string | Lidar |
| `sensor_timestamp` | long | Lidar |
| `ego_translation` | struct | EgoMotion |
| `ego_rotation` | struct | EgoMotion |
| `extrinsics` | struct | Calibration |

**Partition By**: `clip_id` (bucketed)  
**Sort By**: `sensor_timestamp`

### 3.3 `gold.sensor_fusion_frame`

**Purpose**: Multi-modal synchronized frame for sensor fusion training

| Column | Type | Source |
|--------|------|--------|
| `frame_id` | string | Frame |
| `camera_data` | map<string, struct> | Camera (all cameras) |
| `lidar_filename` | string | Lidar |
| `radar_data` | map<string, struct> | Radar (all radars) |
| `annotations` | array<struct> | DynamicObject |
| `occupancy` | struct | Occupancy |
| `motion` | struct | Motion |

**Partition By**: `clip_id` (bucketed)

---

## 4. Implementation Architecture

### 4.1 Ingestion Pipeline Components

```
┌─────────────────────────────────────────────────────────────────────┐
│                      KAIST Ingestion Pipeline                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │   Source     │    │   Bronze     │    │   Silver     │          │
│  │   (JSON/     │───▶│   Layer      │───▶│   Layer      │          │
│  │   Files)     │    │   (Raw)      │    │   (Clean)    │          │
│  └──────────────┘    └──────────────┘    └──────────────┘          │
│                                                 │                    │
│                                                 ▼                    │
│                                          ┌──────────────┐           │
│                                          │   Gold       │           │
│                                          │   Layer      │           │
│                                          │   (ML-Ready) │           │
│                                          └──────────────┘           │
│                                                                      │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │                    Supporting Infrastructure                  │   │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │   │
│  │  │ Schema  │  │ Quality │  │ Metrics │  │ Lineage │        │   │
│  │  │ Registry│  │ Checks  │  │ Monitor │  │ Tracker │        │   │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.2 File Structure

```
python-scripts/
├── kaist/
│   ├── __init__.py
│   ├── config.py                 # Environment & Spark configuration
│   ├── schemas.py                # PySpark schema definitions
│   ├── ingest_bronze.py          # Bronze layer ingestion
│   ├── transform_silver.py       # Silver layer transformations
│   ├── build_gold.py             # Gold table generation
│   ├── validators.py             # Data quality checks
│   └── utils/
│       ├── __init__.py
│       ├── spark_utils.py        # Reusable Spark utilities
│       └── s3_utils.py           # S3/Ceph path handling
│
├── kaist_runner.py               # CLI entry point
└── ingest_nuscenes_mini.py       # (existing)
```

---

## 5. Detailed Implementation Plan

### Phase 1: Infrastructure Setup (Week 1)

#### 1.1 Create Base Configuration Module
- [ ] Port existing Spark builder pattern from `ingest_nuscenes_mini.py`
- [ ] Add Ceph-specific S3 configuration options
- [ ] Create namespace management utilities

#### 1.2 Define Schema Contracts
- [ ] Create PySpark `StructType` definitions for all KAIST tables
- [ ] Implement schema validation functions
- [ ] Create schema evolution handlers

### Phase 2: Bronze Layer Ingestion (Week 1-2)

#### 2.1 Core Tables Ingestion
- [ ] Session, Clip, Frame hierarchy
- [ ] Category reference table
- [ ] HDMap table

#### 2.2 Sensor Tables Ingestion
- [ ] Camera table (with multi-sensor support)
- [ ] Lidar table
- [ ] Radar table (with multi-sensor support)
- [ ] Calibration table

#### 2.3 Annotation Tables Ingestion
- [ ] DynamicObject table
- [ ] Occupancy table
- [ ] Motion table
- [ ] EgoMotion tables (frame-level and session-level)

### Phase 3: Silver Layer Transformation (Week 2-3)

#### 3.1 Data Cleaning & Typing
- [ ] Implement type coercion for SE3 poses and quaternions
- [ ] Handle nullable fields and defaults
- [ ] Deduplicate records

#### 3.2 Partitioning & Optimization
- [ ] Apply partition strategies per table
- [ ] Configure sort orders
- [ ] Optimize file sizes (target 128-256 MB per file)

### Phase 4: Gold Layer Construction (Week 3-4)

#### 4.1 Pre-Join Pipelines
- [ ] Build `camera_annotations` gold table
- [ ] Build `lidar_with_ego` gold table
- [ ] Build `sensor_fusion_frame` gold table

#### 4.2 Performance Validation
- [ ] Benchmark query performance against silver tables
- [ ] Validate partition pruning effectiveness
- [ ] Test with realistic ML data loading patterns

### Phase 5: Automation & Monitoring (Week 4-5)

#### 5.1 Scheduling
- [ ] Create Docker-based runner for ingestion jobs
- [ ] Add support for incremental ingestion
- [ ] Implement idempotent upsert logic

#### 5.2 Observability
- [ ] Integrate row-count metrics
- [ ] Add data freshness monitoring
- [ ] Create Superset dashboards for ingestion status

---

## 6. Ceph-Specific Considerations

Since the production lakehouse will use **Ceph** instead of MinIO:

### 6.1 Configuration Changes

```python
# Ceph S3 Gateway configuration
CEPH_CONFIG = {
    "s3.endpoint": "http://ceph-rgw:7480",  # Ceph RADOS Gateway
    "s3.path-style-access": "true",
    "s3.region": "default",  # Ceph uses 'default' region typically
}
```

### 6.2 Performance Tuning

- **Object size**: Target 64-256 MB objects for Ceph RADOS efficiency
- **Erasure coding**: Consider bucket policies for cold data tiers
- **Parallel uploads**: Tune `fs.s3a.threads.max` for Ceph bandwidth

### 6.3 Compatibility Testing

- [ ] Verify Iceberg S3FileIO works with Ceph RGW
- [ ] Test multipart upload thresholds
- [ ] Validate SSE (Server-Side Encryption) if required

---

## 7. Data Quality Framework

### 7.1 Validation Rules

| Table | Rule | Severity |
|-------|------|----------|
| All | Primary key uniqueness | CRITICAL |
| Frame | `frame_id` references valid `clip_id` | CRITICAL |
| Camera/Lidar/Radar | `frame_id` exists in Frame table | CRITICAL |
| DynamicObject | `boxes_3d` array is non-empty | WARNING |
| Calibration | `extrinsics` has valid SE3 structure | CRITICAL |
| EgoMotion | Quaternion rotation is normalized | WARNING |

### 7.2 Quality Metrics

```python
@dataclass
class IngestionMetrics:
    table_name: str
    rows_ingested: int
    rows_failed: int
    null_rate: Dict[str, float]  # per-column null percentage
    distinct_key_count: int
    ingestion_duration_seconds: float
```

---

## 8. Appendix: SQL Examples

### 8.1 Create Bronze Tables

```sql
-- Create namespace
CREATE NAMESPACE IF NOT EXISTS kaist.bronze;

-- Example: Frame table
CREATE TABLE IF NOT EXISTS kaist.bronze.frame (
    frame_id STRING,
    clip_id STRING,
    frame_idx INT,
    sensor_timestamps ARRAY<BIGINT>
) USING iceberg
TBLPROPERTIES ('format-version' = '2');
```

### 8.2 Create Silver Tables with Partitioning

```sql
CREATE TABLE kaist.silver.camera (
    frame_id STRING,
    clip_id STRING,
    system_timestamp BIGINT,
    sensor_timestamp BIGINT,
    camera_name STRING,
    filename STRING
) USING iceberg
PARTITIONED BY (camera_name, clip_id)
TBLPROPERTIES (
    'format-version' = '2',
    'write.target-file-size-bytes' = '134217728'  -- 128 MB
);
```

### 8.3 Build Gold Table via Spark SQL

```sql
CREATE TABLE kaist.gold.camera_annotations
USING iceberg
PARTITIONED BY (camera_name)
AS
SELECT 
    f.frame_id,
    f.clip_id,
    c.session_id,
    cam.sensor_timestamp,
    cam.camera_name,
    cam.filename,
    cal.extrinsics,
    cal.camera_intrinsics,
    COLLECT_LIST(STRUCT(
        dyn.boxes_3d,
        cat.category
    )) AS annotations,
    hd.city
FROM kaist.silver.frame f
JOIN kaist.silver.clip c ON f.clip_id = c.clip_id
JOIN kaist.silver.camera cam ON f.frame_id = cam.frame_id
JOIN kaist.silver.calibration cal 
    ON f.clip_id = cal.clip_id AND cam.camera_name = cal.sensor_name
LEFT JOIN kaist.silver.dynamic_object dyn ON f.frame_id = dyn.frame_id
LEFT JOIN kaist.bronze.category cat ON dyn.category_id = cat.category
LEFT JOIN kaist.silver.hdmap hd ON f.clip_id = hd.clip_id
GROUP BY 
    f.frame_id, f.clip_id, c.session_id, cam.sensor_timestamp,
    cam.camera_name, cam.filename, cal.extrinsics, 
    cal.camera_intrinsics, hd.city;
```

---

## 9. Next Steps

1. **Review this plan** with the KAIST institute to validate schema interpretation
2. **Create skeleton code** for the ingestion pipeline
3. **Set up test environment** with sample KAIST data
4. **Iterate on gold table designs** based on actual ML workload requirements

---

*Document Version: 1.0*  
*Created: 2026-02-06*  
*Author: Automated Analysis*

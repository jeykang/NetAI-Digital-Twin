{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b647faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Baseline: Pure Python] 실험 시작\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 데이터 경로\n",
    "RAW_DATA_PATH = \"/home/user/kkr/v1.0-mini/v1.0-mini\"\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(os.path.join(RAW_DATA_PATH, filename), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "print(\">>> [Baseline: Pure Python] 실험 시작\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ce036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exp_data(SCALE_FACTOR):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 1. Initialization\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # print(\"Step 1: Loading JSON files into memory...\")\n",
    "\n",
    "    # 파일 로드 (instance.json 추가)\n",
    "    data_samples = load_json('sample.json')\n",
    "    data_sample_data = load_json('sample_data.json')\n",
    "    data_annotations = load_json('sample_annotation.json')\n",
    "    data_categories = load_json('category.json')\n",
    "    data_instances = load_json('instance.json')  # [추가됨]\n",
    "    data_sensors = load_json('sensor.json')\n",
    "    data_calibrated_sensors = load_json('calibrated_sensor.json')\n",
    "\n",
    "    # print(f\">>> [Experiment] Scaling Data by {SCALE_FACTOR}x ...\")\n",
    "\n",
    "    # 리스트 단순 복제 (메모리 사용량도 정직하게 늘어남)\n",
    "    data_samples = data_samples * SCALE_FACTOR\n",
    "    data_sample_data = data_sample_data * SCALE_FACTOR\n",
    "    data_annotations = data_annotations * SCALE_FACTOR\n",
    "\n",
    "    # print(\"Step 2: Building Index (Token mapping)...\")\n",
    "\n",
    "    # List를 Dictionary로 변환하여 인덱스 생성\n",
    "    sample_map = {s['token']: s for s in data_samples}\n",
    "    # category.json의 \n",
    "    category_map = {c['token']: c['name'] for c in data_categories}\n",
    "\n",
    "    # [추가됨] Instance Token -> Category Token 맵핑\n",
    "    # Annotation은 category_token을 직접 안 가지고 instance_token만 가짐\n",
    "    instance_to_category_map = {i['token']: i['category_token'] for i in data_instances}\n",
    "\n",
    "    # Sensor & Channel 맵핑: calibrated_sensor_token -> channel(RADAR_FRONT)\n",
    "    sensor_channel_map = {s['token']: s['channel'] for s in data_sensors}\n",
    "    calib_to_channel_map = {}\n",
    "    for cs in data_calibrated_sensors:\n",
    "        s_token = cs['sensor_token']\n",
    "        if s_token in sensor_channel_map:\n",
    "            calib_to_channel_map[cs['token']] = sensor_channel_map[s_token]\n",
    "\n",
    "    # Annotation Grouping\n",
    "    ann_by_sample = {} \n",
    "    for ann in data_annotations:\n",
    "        s_token = ann['sample_token']\n",
    "        if s_token not in ann_by_sample:\n",
    "            ann_by_sample[s_token] = []\n",
    "        ann_by_sample[s_token].append(ann)\n",
    "    \n",
    "    return data_sample_data, calib_to_channel_map, ann_by_sample, instance_to_category_map, category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(data_sample_data, calib_to_channel_map, ann_by_sample, instance_to_category_map, category_map, test_count=15):\n",
    "    all_time=0\n",
    "    # print(\"Step 3: Filtering Data (CAM_FRONT & Pedestrian)...\")\n",
    "    for i in range(test_count):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Data Lookup & Filtering\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "        dataset_pairs = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for sd in data_sample_data:\n",
    "            # Channel 확인 (sensor 장비 + 방향 확인)\n",
    "            calib_token = sd['calibrated_sensor_token']\n",
    "            channel = calib_to_channel_map.get(calib_token)\n",
    "            \n",
    "            if channel != 'CAM_FRONT':\n",
    "                continue\n",
    "            \n",
    "            s_token = sd['sample_token']\n",
    "            \n",
    "            if s_token in ann_by_sample:\n",
    "                for ann in ann_by_sample[s_token]:\n",
    "                    # [수정됨] Annotation -> Instance -> Category 연결\n",
    "                    inst_token = ann['instance_token']\n",
    "                    cat_token = instance_to_category_map.get(inst_token)\n",
    "                    cat_name = category_map.get(cat_token)\n",
    "                    \n",
    "                    if cat_name == 'human.pedestrian.adult':\n",
    "                        dataset_pairs.append({\n",
    "                            'img_path': sd['filename'],\n",
    "                            'bbox_translation': ann['translation'],\n",
    "                            'bbox_size': ann['size'],\n",
    "                            'bbox_rotation': ann['rotation']\n",
    "                        })\n",
    "            else:\n",
    "                print(f\"Warning: No annotations for sample_token {s_token}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Report\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # print(\"-\" * 30)\n",
    "        # print(f\"Total Time Elapsed: {end_time - start_time:.4f} sec\")\n",
    "        # print(\"-\" * 30)\n",
    "        all_time += end_time - start_time\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Num_runs:\", test_count)\n",
    "    print(f\"Total Rows Found: {len(dataset_pairs)}\")\n",
    "    print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9d3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Num_runs: 100\n",
      "Total Rows Found: 27483\n",
      "Total avg Time Elapsed: 0.0273 sec\n",
      "------------------------------\n",
      "------------------------------\n",
      "Num_runs: 100\n",
      "Total Rows Found: 247347\n",
      "Total avg Time Elapsed: 0.2350 sec\n",
      "------------------------------\n",
      "------------------------------\n",
      "Num_runs: 100\n",
      "Total Rows Found: 687075\n",
      "Total avg Time Elapsed: 0.6232 sec\n",
      "------------------------------\n",
      "------------------------------\n",
      "Num_runs: 100\n",
      "Total Rows Found: 1346667\n",
      "Total avg Time Elapsed: 1.1253 sec\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    re = make_exp_data(SCALE_FACTOR=sf) # 1*!=1\n",
    "    exp(*re, test_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18450fb9",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 데이터 경로\n",
    "RAW_DATA_PATH = \"/home/user/kkr/v1.0-mini/v1.0-mini\"\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(os.path.join(RAW_DATA_PATH, filename), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "print(\">>> [Baseline: Pure Python] 실험 시작\")\n",
    "\n",
    "def make_exp_data(SCALE_FACTOR):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 1. Initialization\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # print(\"Step 1: Loading JSON files into memory...\")\n",
    "\n",
    "    # 파일 로드 (instance.json 추가)\n",
    "    data_samples = load_json('sample.json')\n",
    "    data_sample_data = load_json('sample_data.json')\n",
    "    data_annotations = load_json('sample_annotation.json')\n",
    "    data_categories = load_json('category.json')\n",
    "    data_instances = load_json('instance.json')  # [추가됨]\n",
    "    data_sensors = load_json('sensor.json')\n",
    "    data_calibrated_sensors = load_json('calibrated_sensor.json')\n",
    "\n",
    "    # print(f\">>> [Experiment] Scaling Data by {SCALE_FACTOR}x ...\")\n",
    "\n",
    "    # 리스트 단순 복제 (메모리 사용량도 정직하게 늘어남)\n",
    "    data_samples = data_samples * SCALE_FACTOR\n",
    "    data_sample_data = data_sample_data * SCALE_FACTOR\n",
    "    data_annotations = data_annotations * SCALE_FACTOR\n",
    "\n",
    "    # print(\"Step 2: Building Index (Token mapping)...\")\n",
    "\n",
    "    # List를 Dictionary로 변환하여 인덱스 생성\n",
    "    sample_map = {s['token']: s for s in data_samples}\n",
    "    # category.json의 \n",
    "    category_map = {c['token']: c['name'] for c in data_categories}\n",
    "\n",
    "    # [추가됨] Instance Token -> Category Token 맵핑\n",
    "    # Annotation은 category_token을 직접 안 가지고 instance_token만 가짐\n",
    "    instance_to_category_map = {i['token']: i['category_token'] for i in data_instances}\n",
    "\n",
    "    # Sensor & Channel 맵핑: calibrated_sensor_token -> channel(RADAR_FRONT)\n",
    "    sensor_channel_map = {s['token']: s['channel'] for s in data_sensors}\n",
    "    calib_to_channel_map = {}\n",
    "    for cs in data_calibrated_sensors:\n",
    "        s_token = cs['sensor_token']\n",
    "        if s_token in sensor_channel_map:\n",
    "            calib_to_channel_map[cs['token']] = sensor_channel_map[s_token]\n",
    "\n",
    "    # Annotation Grouping\n",
    "    ann_by_sample = {} \n",
    "    for ann in data_annotations:\n",
    "        s_token = ann['sample_token']\n",
    "        if s_token not in ann_by_sample:\n",
    "            ann_by_sample[s_token] = []\n",
    "        ann_by_sample[s_token].append(ann)\n",
    "    \n",
    "    return data_sample_data, calib_to_channel_map, ann_by_sample, instance_to_category_map, category_map\n",
    "\n",
    "def exp(data_sample_data, calib_to_channel_map, ann_by_sample, instance_to_category_map, category_map, test_count=15):\n",
    "    all_time=0\n",
    "    # print(\"Step 3: Filtering Data (CAM_FRONT & Pedestrian)...\")\n",
    "    for i in range(test_count):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Data Lookup & Filtering\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "        dataset_pairs = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for sd in data_sample_data:\n",
    "            # Channel 확인 (sensor 장비 + 방향 확인)\n",
    "            calib_token = sd['calibrated_sensor_token']\n",
    "            channel = calib_to_channel_map.get(calib_token)\n",
    "            \n",
    "            if channel != 'CAM_FRONT':\n",
    "                continue\n",
    "            \n",
    "            s_token = sd['sample_token']\n",
    "            \n",
    "            # if s_token in ann_by_sample:\n",
    "            for ann in ann_by_sample[s_token]:\n",
    "                # [수정됨] Annotation -> Instance -> Category 연결\n",
    "                inst_token = ann['instance_token']\n",
    "                cat_token = instance_to_category_map.get(inst_token)\n",
    "                cat_name = category_map.get(cat_token)\n",
    "                \n",
    "                if cat_name == 'human.pedestrian.adult':\n",
    "                    dataset_pairs.append({\n",
    "                        'img_path': sd['filename'],\n",
    "                        'bbox_translation': ann['translation'],\n",
    "                        'bbox_size': ann['size'],\n",
    "                        'bbox_rotation': ann['rotation']\n",
    "                    })\n",
    "            # else:\n",
    "            #     print(f\"Warning: No annotations for sample_token {s_token}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Report\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # print(\"-\" * 30)\n",
    "        # print(f\"Total Time Elapsed: {end_time - start_time:.4f} sec\")\n",
    "        # print(\"-\" * 30)\n",
    "        all_time += end_time - start_time\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Num_runs:\", test_count)\n",
    "    print(f\"Total Rows Found: {len(dataset_pairs)}\")\n",
    "    print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    re = make_exp_data(SCALE_FACTOR=sf) # 1*!=1\n",
    "    exp(*re, test_count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

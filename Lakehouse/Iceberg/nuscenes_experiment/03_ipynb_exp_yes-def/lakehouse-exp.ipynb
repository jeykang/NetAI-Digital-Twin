{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a192eda6",
   "metadata": {},
   "source": [
    "# Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99aa5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# [PART 1] Environment & Spark Init\n",
    "# =============================================================================\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "s3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio:9000\")\n",
    "nessie_uri = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v1\")\n",
    "RAW_DATA_PATH = \"/home/user/kkr/v1.0-mini/v1.0-mini\"\n",
    "\n",
    "if not aws_access_key or not aws_secret_key:\n",
    "    print(\"Error: AWS Access Key or Secret Key is missing in environment variables.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NessieMinioSpark\") \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.defaultCatalog', 'spark_catalog') \\\n",
    "    .config('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.nessie.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.nessie.ref', 'main') \\\n",
    "    .config('spark.sql.catalog.nessie.cache-enabled', 'false') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.region', aws_region) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.access-key-id', aws_access_key) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.secret-access-key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', aws_access_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.endpoint', s3_endpoint) \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672314b6",
   "metadata": {},
   "source": [
    "## Iceberg-Silver Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28ce1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exp_data(spark, SCALE_FACTOR):\n",
    "\n",
    "    # setup_start = time.time()\n",
    "\n",
    "    # Namespace 생성\n",
    "    db_name = f\"nusc_db{SCALE_FACTOR}\"\n",
    "    spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{db_name}\")\n",
    "\n",
    "    try:\n",
    "        # 테이블이 이미 있는지 확인 (테스트용)\n",
    "        # spark.table(f\"nessie.{db_name}.sample_data\") # 이 부분은 주석 처리하거나 에러 핸들링을 위해 둠\n",
    "        # print(\"Tables might exist. Attempting to overwrite...\")\n",
    "        pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 1. JSON 읽기\n",
    "    df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "    df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "    df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "    df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "    df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "    df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "    df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "    # 2. Pre-Join (Denormalization)\n",
    "    df_channel_map = df_calibrated.join(\n",
    "        df_sensor, \n",
    "        df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]\n",
    "    ).select(\n",
    "        df_calibrated[\"token\"].alias(\"calib_token\"), \n",
    "        df_sensor[\"channel\"]\n",
    "    )\n",
    "\n",
    "    df_sample_data_enriched = df_sample_data.join(\n",
    "        df_channel_map,\n",
    "        df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]\n",
    "    ).drop(\"calib_token\")\n",
    "\n",
    "    # =========================================================\n",
    "    # [실험 변수] 데이터 스케일 팩터\n",
    "    # =========================================================\n",
    "\n",
    "    def scale_df(df, factor):\n",
    "        if factor <= 1: return df\n",
    "        return df.crossJoin(spark.range(factor)).drop(\"id\")\n",
    "\n",
    "    print(f\">>> [Experiment] Scaling Key Tables by {SCALE_FACTOR}x (others keep 1x) ...\")\n",
    "\n",
    "    # 1. 참조 테이블 (Reference Tables) - 스케일링 하지 않음 (Python Dictionary 동작 모사)\n",
    "    # samples, instances, category는 1배 유지\n",
    "    df_sample.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.samples\")\n",
    "    df_category.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.category\")\n",
    "    df_instance.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.instances\")\n",
    "\n",
    "    # 2. 팩트 테이블 (Fact Tables) - 스케일링 적용 (데이터 폭증 유발)\n",
    "    # sample_data와 annotations만 늘려서 10 * 10 = 100배 효과를 냄\n",
    "    scale_df(df_sample_data_enriched, SCALE_FACTOR).write.format(\"iceberg\") \\\n",
    "        .partitionBy(\"channel\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"nessie.{db_name}.sample_data\")\n",
    "\n",
    "    scale_df(df_annotation, SCALE_FACTOR).write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.annotations\")\n",
    "\n",
    "    # print(f\"Data Ingestion Finished: {time.time() - setup_start:.2f}s\")\n",
    "\n",
    "    return db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_s(db_name, test_count=15):\n",
    "    all_time=0\n",
    "    for i in range(test_count):\n",
    "        query_start = time.time()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            sd.filename as img_path,\n",
    "            a.translation,\n",
    "            a.size,\n",
    "            a.rotation\n",
    "        FROM nessie.{db_name}.samples s\n",
    "        JOIN nessie.{db_name}.sample_data sd \n",
    "            ON s.token = sd.sample_token\n",
    "        JOIN nessie.{db_name}.annotations a \n",
    "            ON s.token = a.sample_token\n",
    "        JOIN nessie.{db_name}.instances i \n",
    "            ON a.instance_token = i.token\n",
    "        JOIN nessie.{db_name}.category c \n",
    "            ON i.category_token = c.token\n",
    "        WHERE \n",
    "            sd.channel = 'CAM_FRONT' \n",
    "            AND c.name = 'human.pedestrian.adult'\n",
    "        \"\"\"\n",
    "\n",
    "        result_df = spark.sql(query)\n",
    "        count = result_df.count()\n",
    "\n",
    "        query_end = time.time()\n",
    "\n",
    "        all_time += query_end - query_start\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")        \n",
    "    print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7383cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Experiment] Scaling Key Tables by 1x (others keep 1x) ...\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      "Total avg Time Elapsed: 0.1613 sec\n",
      ">>> [Experiment] Scaling Key Tables by 3x (others keep 1x) ...\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      "Total avg Time Elapsed: 0.2279 sec\n",
      ">>> [Experiment] Scaling Key Tables by 5x (others keep 1x) ...\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      "Total avg Time Elapsed: 0.3634 sec\n",
      ">>> [Experiment] Scaling Key Tables by 7x (others keep 1x) ...\n",
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      "Total avg Time Elapsed: 0.5480 sec\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    db_name = make_exp_data(spark, SCALE_FACTOR=sf)\n",
    "    exp_s(db_name=db_name, test_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b53f3e",
   "metadata": {},
   "source": [
    "### Iceberg Silver Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# [PART 1] Environment & Spark Init\n",
    "# =============================================================================\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "s3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio:9000\")\n",
    "nessie_uri = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v1\")\n",
    "RAW_DATA_PATH = \"/home/user/kkr/v1.0-mini/v1.0-mini\"\n",
    "\n",
    "if not aws_access_key or not aws_secret_key:\n",
    "    print(\"Error: AWS Access Key or Secret Key is missing in environment variables.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NessieMinioSpark\") \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.defaultCatalog', 'spark_catalog') \\\n",
    "    .config('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.nessie.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.nessie.ref', 'main') \\\n",
    "    .config('spark.sql.catalog.nessie.cache-enabled', 'false') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.region', aws_region) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.access-key-id', aws_access_key) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.secret-access-key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', aws_access_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.endpoint', s3_endpoint) \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def make_exp_data(spark, SCALE_FACTOR):\n",
    "\n",
    "    # setup_start = time.time()\n",
    "\n",
    "    # Namespace 생성\n",
    "    db_name = f\"nusc_db{SCALE_FACTOR}\"\n",
    "    spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{db_name}\")\n",
    "\n",
    "    try:\n",
    "        # 테이블이 이미 있는지 확인 (테스트용)\n",
    "        # spark.table(f\"nessie.{db_name}.sample_data\") # 이 부분은 주석 처리하거나 에러 핸들링을 위해 둠\n",
    "        # print(\"Tables might exist. Attempting to overwrite...\")\n",
    "        pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 1. JSON 읽기\n",
    "    df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "    df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "    df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "    df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "    df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "    df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "    df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "    # 2. Pre-Join (Denormalization)\n",
    "    df_channel_map = df_calibrated.join(\n",
    "        df_sensor, \n",
    "        df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]\n",
    "    ).select(\n",
    "        df_calibrated[\"token\"].alias(\"calib_token\"), \n",
    "        df_sensor[\"channel\"]\n",
    "    )\n",
    "\n",
    "    df_sample_data_enriched = df_sample_data.join(\n",
    "        df_channel_map,\n",
    "        df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]\n",
    "    ).drop(\"calib_token\")\n",
    "\n",
    "    # =========================================================\n",
    "    # [실험 변수] 데이터 스케일 팩터\n",
    "    # =========================================================\n",
    "\n",
    "    def scale_df(df, factor):\n",
    "        if factor <= 1: return df\n",
    "        return df.crossJoin(spark.range(factor)).drop(\"id\")\n",
    "\n",
    "    print(f\">>> [Experiment] Scaling Key Tables by {SCALE_FACTOR}x (others keep 1x) ...\")\n",
    "\n",
    "    # 1. 참조 테이블 (Reference Tables) - 스케일링 하지 않음 (Python Dictionary 동작 모사)\n",
    "    # samples, instances, category는 1배 유지\n",
    "    df_sample.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.samples\")\n",
    "    df_category.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.category\")\n",
    "    df_instance.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.instances\")\n",
    "\n",
    "    # 2. 팩트 테이블 (Fact Tables) - 스케일링 적용 (데이터 폭증 유발)\n",
    "    # sample_data와 annotations만 늘려서 10 * 10 = 100배 효과를 냄\n",
    "    scale_df(df_sample_data_enriched, SCALE_FACTOR).write.format(\"iceberg\") \\\n",
    "        .partitionBy(\"channel\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"nessie.{db_name}.sample_data\")\n",
    "\n",
    "    scale_df(df_annotation, SCALE_FACTOR).write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"nessie.{db_name}.annotations\")\n",
    "\n",
    "    # print(f\"Data Ingestion Finished: {time.time() - setup_start:.2f}s\")\n",
    "\n",
    "    return db_name\n",
    "\n",
    "def exp_s(db_name, test_count=15):\n",
    "    all_time=0\n",
    "    for i in range(test_count):\n",
    "        query_start = time.time()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            sd.filename as img_path,\n",
    "            a.translation,\n",
    "            a.size,\n",
    "            a.rotation\n",
    "        FROM nessie.{db_name}.samples s\n",
    "        JOIN nessie.{db_name}.sample_data sd \n",
    "            ON s.token = sd.sample_token\n",
    "        JOIN nessie.{db_name}.annotations a \n",
    "            ON s.token = a.sample_token\n",
    "        JOIN nessie.{db_name}.instances i \n",
    "            ON a.instance_token = i.token\n",
    "        JOIN nessie.{db_name}.category c \n",
    "            ON i.category_token = c.token\n",
    "        WHERE \n",
    "            sd.channel = 'CAM_FRONT' \n",
    "            AND c.name = 'human.pedestrian.adult'\n",
    "        \"\"\"\n",
    "\n",
    "        result_df = spark.sql(query)\n",
    "        count = result_df.count()\n",
    "\n",
    "        query_end = time.time()\n",
    "\n",
    "        all_time += query_end - query_start\n",
    "    print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")        \n",
    "    print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\")    \n",
    "\n",
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    db_name = make_exp_data(spark, SCALE_FACTOR=sf)\n",
    "    exp_s(db_name=db_name, test_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7fe75",
   "metadata": {},
   "source": [
    "## Iceberg-Gold Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabf893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] Phase 1: Gold Table 생성 (Join + Denormalization)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\">>> [Lakehouse] Phase 1: Gold Table 생성 (Join + Denormalization)\")\n",
    "\n",
    "# 1. JSON 읽기\n",
    "df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "# 2. 복잡한 조인을 미리 수행 (Denormalization)\n",
    "# CAM_FRONT 채널 정보 결합\n",
    "df_channel_map = df_calibrated.join(df_sensor, df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]) \\\n",
    "    .select(df_calibrated[\"token\"].alias(\"calib_token\"), df_sensor[\"channel\"])\n",
    "\n",
    "# 전체 조인 수행 (Gold Table용 데이터 구성)\n",
    "df_gold_raw = df_sample_data.join(df_channel_map, df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]) \\\n",
    "    .join(df_annotation, df_sample_data[\"sample_token\"] == df_annotation[\"sample_token\"]) \\\n",
    "    .join(df_instance, df_annotation[\"instance_token\"] == df_instance[\"token\"]) \\\n",
    "    .join(df_category, df_instance[\"category_token\"] == df_category[\"token\"]) \\\n",
    "    .select(\n",
    "        df_sample_data[\"filename\"].alias(\"img_path\"),\n",
    "        df_annotation[\"translation\"],\n",
    "        df_annotation[\"size\"],\n",
    "        df_annotation[\"rotation\"],\n",
    "        df_sensor[\"channel\"],\n",
    "        df_category[\"name\"].alias(\"category_name\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6777fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exp_data_g(df_gold_raw, SCALE_FACTOR):\n",
    "    db_name = f\"nusc_dbG{SCALE_FACTOR}\"\n",
    "    spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{db_name}\")\n",
    "    \n",
    "    # 3. 데이터 스케일링 (10배 증강 -> 조인 폭발 시뮬레이션 결과와 맞추기 위해 100배 효과 적용 가능)\n",
    "    def scale_df(df, factor):\n",
    "        if factor <= 1: return df\n",
    "        # Baseline과 동일하게 10x10=100배 효과를 내기 위해 factor*factor로 증강\n",
    "        return df.crossJoin(spark.range(factor * factor)).drop(\"id\")\n",
    "\n",
    "    print(f\">>> [Experiment] Scaling Gold Table by {SCALE_FACTOR}x{SCALE_FACTOR}={SCALE_FACTOR*SCALE_FACTOR}x ...\")\n",
    "    df_gold_final = scale_df(df_gold_raw, SCALE_FACTOR)\n",
    "\n",
    "    # 4. Iceberg Gold Table 저장\n",
    "    # CAM_FRONT나 Category에 상관없이 일단 저장한 뒤 쿼리에서 필터링하는 방식이 실용적입니다.\n",
    "    df_gold_final.write.format(\"iceberg\") \\\n",
    "        .partitionBy(\"channel\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"nessie.{db_name}.gold_train_set\")\n",
    "\n",
    "    # print(f\"Gold Table Ingestion Finished:\")\n",
    "\n",
    "    return db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a2168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_g(db_name, test_count=15):\n",
    "  all_time=0\n",
    "  for i in range(test_count):\n",
    "    # print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "    query_start = time.time()\n",
    "\n",
    "    # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "    query = f\"\"\"\n",
    "    SELECT img_path, translation, size, rotation\n",
    "    FROM nessie.{db_name}.gold_train_set\n",
    "    WHERE channel = 'CAM_FRONT' \n",
    "      AND category_name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    all_time += query_end - query_start\n",
    "\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\\n\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed392c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Experiment] Scaling Gold Table by 1x1=1x ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 27483건 추출 완료\n",
      "Total avg Time Elapsed: 0.0527 sec\n",
      "\n",
      ">>> [Experiment] Scaling Gold Table by 3x3=9x ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 247347건 추출 완료\n",
      "Total avg Time Elapsed: 0.0551 sec\n",
      "\n",
      ">>> [Experiment] Scaling Gold Table by 5x5=25x ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 687075건 추출 완료\n",
      "Total avg Time Elapsed: 0.0740 sec\n",
      "\n",
      ">>> [Experiment] Scaling Gold Table by 7x7=49x ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Lakehouse] 결과: 보행자 데이터 1346667건 추출 완료\n",
      "Total avg Time Elapsed: 0.0799 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    db_name = make_exp_data_g(df_gold_raw, SCALE_FACTOR=sf)\n",
    "    exp_g(db_name=db_name, test_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f65fc",
   "metadata": {},
   "source": [
    "### Iceberg Gold Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# [PART 1] Environment & Spark Init\n",
    "# =============================================================================\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "s3_endpoint = os.getenv(\"AWS_S3_ENDPOINT\", \"http://minio:9000\")\n",
    "nessie_uri = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v1\")\n",
    "RAW_DATA_PATH = \"/home/user/kkr/v1.0-mini/v1.0-mini\"\n",
    "\n",
    "if not aws_access_key or not aws_secret_key:\n",
    "    print(\"Error: AWS Access Key or Secret Key is missing in environment variables.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NessieMinioSpark\") \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.spark_catalog.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.defaultCatalog', 'spark_catalog') \\\n",
    "    .config('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.warehouse', 's3://spark1') \\\n",
    "    .config('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog') \\\n",
    "    .config('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.nessie.uri', nessie_uri) \\\n",
    "    .config('spark.sql.catalog.nessie.ref', 'main') \\\n",
    "    .config('spark.sql.catalog.nessie.cache-enabled', 'false') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.endpoint', s3_endpoint) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.region', aws_region) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.path-style-access', 'true') \\\n",
    "    .config('spark.sql.catalog.nessie.s3.access-key-id', aws_access_key) \\\n",
    "    .config('spark.sql.catalog.nessie.s3.secret-access-key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', aws_access_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', aws_secret_key) \\\n",
    "    .config('spark.hadoop.fs.s3a.endpoint', s3_endpoint) \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\">>> [Lakehouse] Phase 1: Gold Table 생성 (Join + Denormalization)\")\n",
    "\n",
    "# 1. JSON 읽기\n",
    "df_sample = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample.json\")\n",
    "df_sample_data = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_data.json\")\n",
    "df_annotation = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sample_annotation.json\")\n",
    "df_category = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/category.json\")\n",
    "df_instance = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/instance.json\")\n",
    "df_sensor = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/sensor.json\")\n",
    "df_calibrated = spark.read.option(\"multiLine\", True).json(f\"{RAW_DATA_PATH}/calibrated_sensor.json\")\n",
    "\n",
    "# 2. 복잡한 조인을 미리 수행 (Denormalization)\n",
    "# CAM_FRONT 채널 정보 결합\n",
    "df_channel_map = df_calibrated.join(df_sensor, df_calibrated[\"sensor_token\"] == df_sensor[\"token\"]) \\\n",
    "    .select(df_calibrated[\"token\"].alias(\"calib_token\"), df_sensor[\"channel\"])\n",
    "\n",
    "# 전체 조인 수행 (Gold Table용 데이터 구성)\n",
    "df_gold_raw = df_sample_data.join(df_channel_map, df_sample_data[\"calibrated_sensor_token\"] == df_channel_map[\"calib_token\"]) \\\n",
    "    .join(df_annotation, df_sample_data[\"sample_token\"] == df_annotation[\"sample_token\"]) \\\n",
    "    .join(df_instance, df_annotation[\"instance_token\"] == df_instance[\"token\"]) \\\n",
    "    .join(df_category, df_instance[\"category_token\"] == df_category[\"token\"]) \\\n",
    "    .select(\n",
    "        df_sample_data[\"filename\"].alias(\"img_path\"),\n",
    "        df_annotation[\"translation\"],\n",
    "        df_annotation[\"size\"],\n",
    "        df_annotation[\"rotation\"],\n",
    "        df_sensor[\"channel\"],\n",
    "        df_category[\"name\"].alias(\"category_name\")\n",
    "    )\n",
    "\n",
    "def make_exp_data_g(df_gold_raw, SCALE_FACTOR):\n",
    "    db_name = f\"nusc_dbG{SCALE_FACTOR}\"\n",
    "    spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{db_name}\")\n",
    "    \n",
    "    # 3. 데이터 스케일링 (10배 증강 -> 조인 폭발 시뮬레이션 결과와 맞추기 위해 100배 효과 적용 가능)\n",
    "    def scale_df(df, factor):\n",
    "        if factor <= 1: return df\n",
    "        # Baseline과 동일하게 10x10=100배 효과를 내기 위해 factor*factor로 증강\n",
    "        return df.crossJoin(spark.range(factor * factor)).drop(\"id\")\n",
    "\n",
    "    print(f\">>> [Experiment] Scaling Gold Table by {SCALE_FACTOR}x{SCALE_FACTOR}={SCALE_FACTOR*SCALE_FACTOR}x ...\")\n",
    "    df_gold_final = scale_df(df_gold_raw, SCALE_FACTOR)\n",
    "\n",
    "    # 4. Iceberg Gold Table 저장\n",
    "    # CAM_FRONT나 Category에 상관없이 일단 저장한 뒤 쿼리에서 필터링하는 방식이 실용적입니다.\n",
    "    df_gold_final.write.format(\"iceberg\") \\\n",
    "        .partitionBy(\"channel\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"nessie.{db_name}.gold_train_set\")\n",
    "\n",
    "    # print(f\"Gold Table Ingestion Finished:\")\n",
    "\n",
    "    return db_name\n",
    "\n",
    "def exp_g(db_name, test_count=15):\n",
    "  all_time=0\n",
    "  for i in range(test_count):\n",
    "    # print(\">>> [Lakehouse] Phase 2: 실험 시작 - 단일 Gold 테이블 쿼리\")\n",
    "    query_start = time.time()\n",
    "\n",
    "    # 조인이 전혀 없는 단순 필터링 쿼리\n",
    "    query = f\"\"\"\n",
    "    SELECT img_path, translation, size, rotation\n",
    "    FROM nessie.{db_name}.gold_train_set\n",
    "    WHERE channel = 'CAM_FRONT' \n",
    "      AND category_name = 'human.pedestrian.adult'\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = spark.sql(query)\n",
    "    count = result_df.count()\n",
    "\n",
    "    query_end = time.time()\n",
    "    all_time += query_end - query_start\n",
    "\n",
    "  print(f\">>> [Lakehouse] 결과: 보행자 데이터 {count}건 추출 완료\")\n",
    "  print(f\"Total avg Time Elapsed: {all_time/test_count:.4f} sec\\n\")     \n",
    "\n",
    "\n",
    "SCALE_FACTORS = [1, 3, 5, 7]\n",
    "for sf in SCALE_FACTORS:\n",
    "    db_name = make_exp_data_g(df_gold_raw, SCALE_FACTOR=sf)\n",
    "    exp_g(db_name=db_name, test_count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
